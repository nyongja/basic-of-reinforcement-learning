{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Contextual-Policy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlQ64zJQKsxOIWHtFCLlr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyongja/basic-of-reinforcement-learning/blob/main/Contextual_Policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "id": "3d00d0ZNAsE6",
        "outputId": "d3d64c05-93cf-4ca1-ee4b-e946660e8f1e"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 52kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: tensorboard, keras-applications, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwbOy4TH9Ytc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe05cf9-8877-43c4-8511-a2371c8f8359"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqsRLso6AnMl"
      },
      "source": [
        "## 콘텍스트 밴딧\n",
        "각각 손잡이가 4개인 3개의 밴딧을 사용.  \n",
        "각각의 밴딧은 각각의 손잡이에 대해 다른 성공 확률을 가지고 있으므로 최고의 결과를 얻기 위해서는 다른 액션이 요구.  \n",
        "2장의 코드와 유사하게, pullBanadit 함수는 표준정규분포로부터 랜덤한 수를 발생시킴.  \n",
        "구자 낮을수록 양의 보상 반환될 가능성이 큼.  \n",
        "에이전트가 주어진 밴딧에 대해 가장 자주 양의 보상을 주는 손잡이를 항상 선택하는 방법을 학습하는 것이 목표."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAe2OQR7BUo3"
      },
      "source": [
        "class contextual_bandit() :\n",
        "  def __init__(self) :\n",
        "    self.state = 0\n",
        "    # 밴딧들의 손잡이 목록을 작성. 각 밴딧은 각각 손잡이 4, 2, 1이 최적.\n",
        "    self.bandits = np.array([[0.2, 0, -0.0, -5], [0.1, -5, 1, 0.25], [-5, 5, 5, 5]])\n",
        "    self.num_bandits = self.bandits.shape[0]\n",
        "    self.num_actions = self.bandits.shape[1]\n",
        "\n",
        "\n",
        "  def getBandit(self) :\n",
        "    # 각각의 에피소드에 대해 랜덤한 상태를 반환\n",
        "    self.state = np.random.randint(0, len(self.bandits))\n",
        "    return self.state\n",
        "\n",
        "  def pullArm(self, action) :\n",
        "    # 랜덤한 수를 얻는다.\n",
        "    bandit = self.bandits[self.state, action]\n",
        "    result = np.random.randn(1)\n",
        "    if result > bandit :\n",
        "      # 양의 보상을 반환\n",
        "      return 1\n",
        "    else :\n",
        "      # 음의 보상을 반환\n",
        "      return -1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cubyW5nB4mC"
      },
      "source": [
        "## 정책 기반 에이전트\n",
        "입력으로 현재의 상태를 받아 액션을 반환하는 게 전부.  \n",
        "에이전트는 환경의 상태를 조건으로 삼아 액션을 취하게 됨.  \n",
        "에이전트는 1개의 가중치 세트를 이용하는데, 각 가중치의 값은 주어진 밴딧의 특정 손잡이를 선택할 때 반환되는 값의 추정값.  \n",
        "policy gradient를 통해 선택된 액션에 대해 더 큰 보상을 받는 쪽으로 이동하도록 에이전트를 업데이트함.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geXaEByFD4Dh"
      },
      "source": [
        "class agent() :\n",
        "  def __init__(self, lr, s_size, a_size) :\n",
        "    # 네트워크의 피드포워드 부분. 에이전트는 상태를 받아서 액션을 출력\n",
        "    self.state_in = tf.placeholder(shape=[1], dtype = tf.int32)\n",
        "    state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n",
        "    output = slim.fully_connected(state_in_OH, a_size,\n",
        "                                  biases_initializer=None, activation_fn=tf.nn.sigmoid,\n",
        "                                  weights_initializer=tf.ones_initializer())\n",
        "    self.output = tf.reshape(output, [-1])\n",
        "    self.chosen_action = tf.argmax(self.output, 0)\n",
        "\n",
        "    # 학습 과정을 구현함\n",
        "    # 비용을 계산하기 위해 보상과 선택된 액션을 네트워크에 피드하고,\n",
        "    # 네트워크를 업데이트하는 데에 이를 이용함.\n",
        "    self.reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
        "    self.action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "    self.responsible_weight = tf.slice(self.output, self.action_holder, [1])\n",
        "    self.loss = -(tf.log(self.responsible_weight) * self.reward_holder)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = lr)\n",
        "    self.update = optimizer.minimize(self.loss)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "136nyuwCJghF"
      },
      "source": [
        "## 에이전트 학습시키기\n",
        "에이전트는 환경의 상태를 알아내고, 액션을 취하고, 보상을 받음으로써 학습할 것.  \n",
        "이 세가지를 이용해, 주어진 상태에서 시간의 흐름에 따라 최고의 보상을 받을 수 있는 액션을 더 자주 선택할 수 있도록 네트워크를 적절하게 업데이트하는 방법을 알 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyWDL7_DJ6XX",
        "outputId": "49f62543-9854-474c-9e71-00cf53b8c948"
      },
      "source": [
        "# 텐서플로 그래프를 리셋한다.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# 밴딧을 로드한다.\n",
        "cBandit = contextual_bandit()\n",
        "# 에이전트를 로드한다.\n",
        "myAgent = agent(lr = 0.001, s_size = cBandit.num_bandits, a_size = cBandit.num_actions)\n",
        "# 네트워크 내부를 들여다보기 위해 평가할 가중치\n",
        "weights = tf.trainable_variables()[0]\n",
        "\n",
        "# 에이전트를 학습시킬 전체 에피소드 수 설정\n",
        "total_episodes = 10000\n",
        "# 밴딧에 대한 점수판을 0으로 설정\n",
        "total_reward = np.zeros([cBandit.num_bandits, cBandit.num_actions])\n",
        "# 랜덤한 액션을 취할 가능성 설정\n",
        "e = 0.1\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 텐서플로 그래프 론칭\n",
        "with tf.Session() as sess :\n",
        "  sess.run(init)\n",
        "  i = 0\n",
        "  while i < total_episodes :\n",
        "    # 환경으로부터 상태 가져오기\n",
        "    s = cBandit.getBandit()\n",
        "    # 네트워크로부터 랜덤한 액션 또는 하나의 액션을 선택\n",
        "    if np.random.rand(1) < e :\n",
        "      action = np.random.randint(cBandit.num_actions)\n",
        "    else :\n",
        "      action = sess.run(myAgent.chosen_action, feed_dict = {myAgent.state_in:[s]})\n",
        "    \n",
        "    # 주어진 밴딧에 대해 액션을 취한 데 대한 보상을 얻는다.\n",
        "    reward = cBandit.pullArm(action)\n",
        "\n",
        "    # 네트워크를 업데이트한다.\n",
        "    feed_dict = {myAgent.reward_holder : [reward],\n",
        "                 myAgent.action_holder : [action],\n",
        "                 myAgent.state_in : [s]}\n",
        "    _, ww = sess.run([myAgent.update, weights], feed_dict = feed_dict)\n",
        "\n",
        "    # 보상의 총계 업데이트\n",
        "    total_reward[s, action] += reward\n",
        "    if i % 500 == 0 :\n",
        "      print(\"Mean reward for each of the \" + str(cBandit.num_bandits) + \n",
        "            \" bandits : \" + str(np.mean(total_reward, axis = 1)))\n",
        "    i += 1\n",
        "\n",
        "for a in range(cBandit.num_bandits) :\n",
        "  print(\"The agent thinks action \" + str(np.argmax(ww[a]) + 1) + \" for bandit \" + str(a+1) + \" is the most promising....\")\n",
        "  if np.argmax(ww[a]) == np.argmin(cBandit.bandits[a]) :\n",
        "    print(\"...and it was right!\")\n",
        "  else :\n",
        "    print(\"...and it was wrong!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fbb346b4470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fbb346b4470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fbb346b4470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fbb346b4470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Mean reward for each of the 3 bandits : [0.   0.25 0.  ]\n",
            "Mean reward for each of the 3 bandits : [31.75 38.5  33.5 ]\n",
            "Mean reward for each of the 3 bandits : [71.75 79.   65.5 ]\n",
            "Mean reward for each of the 3 bandits : [106.5  119.25 103.  ]\n",
            "Mean reward for each of the 3 bandits : [146.   159.   137.25]\n",
            "Mean reward for each of the 3 bandits : [184.75 194.75 174.25]\n",
            "Mean reward for each of the 3 bandits : [225.75 233.5  206.5 ]\n",
            "Mean reward for each of the 3 bandits : [266.75 272.25 238.25]\n",
            "Mean reward for each of the 3 bandits : [304.75 313.5  273.  ]\n",
            "Mean reward for each of the 3 bandits : [343.75 353.25 306.75]\n",
            "Mean reward for each of the 3 bandits : [384.75 392.   339.  ]\n",
            "Mean reward for each of the 3 bandits : [421.75 428.25 376.25]\n",
            "Mean reward for each of the 3 bandits : [463.5  466.   407.75]\n",
            "Mean reward for each of the 3 bandits : [499.75 502.25 448.25]\n",
            "Mean reward for each of the 3 bandits : [533.75 537.25 488.75]\n",
            "Mean reward for each of the 3 bandits : [570.75 573.   521.5 ]\n",
            "Mean reward for each of the 3 bandits : [608.5  612.   555.75]\n",
            "Mean reward for each of the 3 bandits : [644.   649.5  592.25]\n",
            "Mean reward for each of the 3 bandits : [683.   685.75 629.5 ]\n",
            "Mean reward for each of the 3 bandits : [718.   724.5  666.25]\n",
            "The agent thinks action 4 for bandit 1 is the most promising....\n",
            "...and it was right!\n",
            "The agent thinks action 2 for bandit 2 is the most promising....\n",
            "...and it was right!\n",
            "The agent thinks action 1 for bandit 3 is the most promising....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}