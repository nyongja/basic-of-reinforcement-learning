{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple-Policy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgLEaqxG+UO2LPi6txu60h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "N_MfDUiBtRUP",
        "outputId": "d167821f-6ae2-4e8e-d752-afccc8f3ff03"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 30.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZDSPcUwtLfo",
        "outputId": "087de362-e915-4575-dc19-dc263cd750b5"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow.contrib.slim as slim\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oepKiL6TwcV6"
      },
      "source": [
        "## The Bandit\n",
        "\n",
        "손잡이가 4개인 밴딧을 만들어보자.  \n",
        "pullBandit : 랜덤한 포준정규분포 값을 하나 생성하고 인수로 받은 수보다 크면 1, 아니면 -1을 반환.   \n",
        "따라서 인수로 넘겨지는 수가 작을수록 (+)의 보상이 돌아올 가능성이 높음.  \n",
        "에이전트가 언제나 (+)의 보상을 가져올 손잡이를 선택하도록 학습하는 것이 목표."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FEWnNtlzzBQ"
      },
      "source": [
        "# 밴딧의 손잡이 목록을 작성한다.\n",
        "# 현재 손잡이 4가 가장 자주 양의 보상을 제공하도록 설정되어 있다.\n",
        "bandit_arms = [0.2, 0, -0.2, -2]\n",
        "num_arms = len(bandit_arms)\n",
        "\n",
        "def pullBandit(bandit) :\n",
        "  # 랜덤한 값을 구한다\n",
        "  result = np.random.randn(1)\n",
        "  if result > bandit :\n",
        "    # 양의 보상을 반환\n",
        "    return 1\n",
        "  else :\n",
        "    # 음의 보상을 반환\n",
        "    return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTActgV1wqwn"
      },
      "source": [
        "## 에이전트\n",
        "\n",
        "간단한 신경망을 구현해보자.  \n",
        "각 밴딧 손잡이에 대한 일련의 값들로 구성되어 있고, 여기서 각 값은 해당 밴딧을 선택할 때 반환되는 보상의 추정값을 의미함.  \n",
        "policy gradient를 이용해 선택된 액션에 대한 보상이 큰 쪽으로 이동해가며 에이전트 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s61DefM0iLw"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# 네트워크의 feedforward 부분 구현\n",
        "weights = tf.Variable(tf.ones([num_arms]))\n",
        "output = tf.nn.softmax(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u59reSf0x65"
      },
      "source": [
        "# 학습 과정을 구현한다.\n",
        "# 보상과 선택된 액션을 네트워크에 피드해줌으로써 비용을 계산하고\n",
        "# 비용을 이용해 네트워크 업데이트\n",
        "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32) # placeholder : 텐서를 만들긴 하지만 값을 넣진않음(미리 생성 후 값은 나중에 넣기 위해)\n",
        "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
        "\n",
        "responsible_output = tf.slice(output, action_holder, [1])\n",
        "loss = -(tf.log(responsible_output)*reward_holder)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "update = optimizer.minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTV0Pzm04iW"
      },
      "source": [
        "## 에이전트 학습시키기\n",
        "\n",
        "환경 내에서 에이전트는 액션을 취함으로써 학습하고 보상을 받음.  \n",
        "보상과 액션을 이용하여 시간의 경과에 따라 최고의 보상을 받게 될 액션을 더 자주 선택하려면 네트워크를 어떻게 업데이트해나갈지에 대해 알 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlJrfs7e2EyN",
        "outputId": "ba1a2f94-761e-40ca-c49c-9ad503fa3595"
      },
      "source": [
        "# 에이전트를 학습시킬 총 에피소드의 수를 설정\n",
        "total_episodes = 1000\n",
        "# 밴딧 손잡이에 대한 점수판을 0으로 설정\n",
        "total_reward = np.zeros(num_arms)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 텐서플로 그래프를 론칭\n",
        "with tf.Session() as sess :\n",
        "  sess.run(init)\n",
        "  i = 0\n",
        "  while i < total_episodes :\n",
        "    # 볼츠만 분포에 따라 액션 선택\n",
        "    actions = sess.run(output)\n",
        "    a = np.random.choice(actions, p = actions)\n",
        "    action = np.argmax(actions == a)\n",
        "\n",
        "    # 밴딧 손잡이 중 하나를 선택하여 보상 받기\n",
        "    reward = pullBandit(bandit_arms[action])\n",
        "\n",
        "    # 네트워크 업데이트\n",
        "    _, resp, ww = sess.run([update, responsible_output, weights],\n",
        "                           feed_dict = {reward_holder:[reward], action_holder:[action]})\n",
        "    \n",
        "    # 보상의 총계 업데이트\n",
        "    total_reward[action] += reward\n",
        "    if i % 50 == 0 :\n",
        "      print(\"Running reward for the \" + str(num_arms) + \" arms of the bandit : \" + str(total_reward))\n",
        "    i += 1\n",
        "\n",
        "print(\"\\nThe agent thinks arm \" + str(np.argmax(ww) + 1) + \" is the most promising.....\")\n",
        "if np.argmax(ww) == np.argmax(-np.array(bandit_arms)) :\n",
        "  print(\"...and it was right!\")\n",
        "else :\n",
        "  print(\"...and it was wrong!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running reward for the 4 arms of the bandit : [1. 0. 0. 0.]\n",
            "Running reward for the 4 arms of the bandit : [ 6. -5.  2. 12.]\n",
            "Running reward for the 4 arms of the bandit : [ 3. -7.  7. 26.]\n",
            "Running reward for the 4 arms of the bandit : [-2. -8. 12. 35.]\n",
            "Running reward for the 4 arms of the bandit : [  0. -14.  10.  47.]\n",
            "Running reward for the 4 arms of the bandit : [ -2. -12.  15.  62.]\n",
            "Running reward for the 4 arms of the bandit : [-6. -5. 10. 74.]\n",
            "Running reward for the 4 arms of the bandit : [-12.  -2.  14.  91.]\n",
            "Running reward for the 4 arms of the bandit : [-12.   0.  16. 107.]\n",
            "Running reward for the 4 arms of the bandit : [-13.   2.  23. 123.]\n",
            "Running reward for the 4 arms of the bandit : [-17.   7.  30. 141.]\n",
            "Running reward for the 4 arms of the bandit : [-22.   4.  29. 158.]\n",
            "Running reward for the 4 arms of the bandit : [-24.   6.  27. 172.]\n",
            "Running reward for the 4 arms of the bandit : [-29.   6.  27. 187.]\n",
            "Running reward for the 4 arms of the bandit : [-25.   9.  35. 206.]\n",
            "Running reward for the 4 arms of the bandit : [-26.   4.  31. 224.]\n",
            "Running reward for the 4 arms of the bandit : [-27.  -1.  30. 245.]\n",
            "Running reward for the 4 arms of the bandit : [-29.  -7.  28. 267.]\n",
            "Running reward for the 4 arms of the bandit : [-25. -11.  25. 286.]\n",
            "Running reward for the 4 arms of the bandit : [-23.  -8.  26. 306.]\n",
            "\n",
            "The agent thinks arm 4 is the most promising.....\n",
            "...and it was right!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd1GSUxG5mgo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}