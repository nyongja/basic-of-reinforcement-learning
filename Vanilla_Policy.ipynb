{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla-Policy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNywJ5gx0a+9W+WWIlx6Ul4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyongja/basic-of-reinforcement-learning/blob/main/Vanilla_Policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDorA8CmcFSJ",
        "outputId": "4980e428-3286-4644-d28f-3a887b1e213c"
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrMAXhILevvs",
        "outputId": "f84f95c0-722a-455a-ad14-6b678b8305a1"
      },
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 51kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (53.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuO2zOPtfKrv"
      },
      "source": [
        "## 기본적인 정책 경사 에이전트 구현\n",
        "\n",
        "2개 이상의 액션에도 일반화가 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr2zZ7Twe7Vj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plta"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfkdJIUtfQ4D"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DliTjNQWfkxp"
      },
      "source": [
        "오픈AI 짐에서 특정 환경을 불러오는 코드는 다음과 같음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZKclB1Hfhut"
      },
      "source": [
        "env = gym.make('CartPole-v0')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy30TeLYfjzC"
      },
      "source": [
        "## 정책 기반 에이전트\n",
        "다음은 보상 함수 및 에이전트를 구현하는 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj3_knMnftnW"
      },
      "source": [
        "gamma = 0.99\n",
        "\n",
        "def discount_rewards(r) :\n",
        "  # 보상의 1D 실수 배열을 취해서 할인된 보상을 계산\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)) :\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGUHbPGQf9rh"
      },
      "source": [
        "class agent() :\n",
        "  def __init__(self, lr, s_size, a_size, h_size) :\n",
        "    # 네트워크의 피드포워드 부분. 에이전트는 상태를 받아 액션을 출력\n",
        "    self.state_in = tf.placeholder(shape = [None, s_size], dtype = tf.float32)\n",
        "    hidden = slim.fully_connected(self.state_in, h_size, biases_initializer = None,\n",
        "                                  activation_fn = tf.nn.relu)\n",
        "    self.output = slim.fully_connected(hidden, a_size, biases_initializer = None,\n",
        "                                       activation_fn = tf.nn.softmax)\n",
        "    self.chosen_action = tf.argmax(self.output, 1)\n",
        "\n",
        "    # 학습 과정 구현. 비용을 계산하기 위해 보상과 액션을 네트워크에 피드하고,\n",
        "    # 네트워크를 업데이트하는 데에 이를 이용함.\n",
        "    self.reward_holder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
        "    self.action_holder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
        "\n",
        "    self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
        "    self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
        "\n",
        "    self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs) * self.reward_holder)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    self.gradient_holders = []\n",
        "    for idx, var in enumerate(tvars) :\n",
        "      placeholder = tf.placeholder(tf.float32, name = str(idx) + '_holder')\n",
        "      self.gradient_holders.append(placeholder)\n",
        "\n",
        "    self.gradients = tf.gradients(self.loss, tvars)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
        "    self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "081yqP6pg48Y"
      },
      "source": [
        "## 에이전트 학습시키기\n",
        "\n",
        "마지막으로 학습 과정을 구현한 코드.  \n",
        "큰 구조는 앞 장 예제들과 같음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRki5jBpg4dT",
        "outputId": "6c48ec16-3016-498f-b19c-c33fc537b9c3"
      },
      "source": [
        "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
        "\n",
        "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
        "\n",
        "total_episodes = 5000 #Set total number of episodes to train agent on.\n",
        "max_ep = 999\n",
        "update_frequency = 5\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the tensorflow graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    i = 0\n",
        "    total_reward = []\n",
        "    total_length = []\n",
        "        \n",
        "    gradBuffer = sess.run(tf.trainable_variables())\n",
        "    for ix,grad in enumerate(gradBuffer):\n",
        "        gradBuffer[ix] = grad * 0\n",
        "        \n",
        "    while i < total_episodes:\n",
        "        s = env.reset()\n",
        "        running_reward = 0\n",
        "        ep_history = []\n",
        "        for j in range(max_ep):\n",
        "            #Probabilistically pick an action given our network outputs.\n",
        "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
        "            a = np.random.choice(a_dist[0],p=a_dist[0])\n",
        "            a = np.argmax(a_dist == a)\n",
        "\n",
        "            s1,r,d,_ = env.step(a) #Get our reward for taking an action given a bandit.\n",
        "            ep_history.append([s,a,r,s1])\n",
        "            s = s1\n",
        "            running_reward += r\n",
        "            if d == True:\n",
        "                #Update the network.\n",
        "                ep_history = np.array(ep_history)\n",
        "                ep_history[:,2] = discount_rewards(ep_history[:,2])\n",
        "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
        "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
        "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
        "                for idx,grad in enumerate(grads):\n",
        "                    gradBuffer[idx] += grad\n",
        "\n",
        "                if i % update_frequency == 0 and i != 0:\n",
        "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
        "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
        "                    for ix,grad in enumerate(gradBuffer):\n",
        "                        gradBuffer[ix] = grad * 0\n",
        "                \n",
        "                total_reward.append(running_reward)\n",
        "                total_length.append(j)\n",
        "                break\n",
        "\n",
        "        \n",
        "            #Update our running tally of scores.\n",
        "        if i % 100 == 0:\n",
        "            print(np.mean(total_reward[-100:]))\n",
        "        i += 1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae77b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae77b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae7828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae7828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae7828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f47bcae7828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "20.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "29.1\n",
            "33.27\n",
            "36.44\n",
            "44.76\n",
            "78.14\n",
            "120.03\n",
            "152.7\n",
            "166.83\n",
            "174.41\n",
            "184.61\n",
            "174.32\n",
            "179.57\n",
            "189.24\n",
            "195.46\n",
            "194.92\n",
            "196.9\n",
            "196.49\n",
            "196.37\n",
            "198.17\n",
            "198.44\n",
            "199.27\n",
            "200.0\n",
            "199.09\n",
            "198.77\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "198.06\n",
            "198.39\n",
            "193.64\n",
            "179.38\n",
            "179.5\n",
            "197.54\n",
            "200.0\n",
            "199.87\n",
            "199.6\n",
            "200.0\n",
            "200.0\n",
            "200.0\n",
            "199.57\n",
            "195.9\n",
            "172.99\n",
            "179.61\n",
            "186.81\n",
            "174.57\n",
            "183.38\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}